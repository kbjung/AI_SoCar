{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Lab 1-2. HW1 Answer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B34orMykk6CK"
      },
      "source": [
        "## HW1\n",
        "데이터셋의 크기가 1/10이 되었을때도 gradient descent를 이용한 linear regression이 잘 동작하는지 강의에서 배운 k-fold cross validation (k=5)을 활용해서 확인해보세요. \n",
        "그리고 데이터셋의 크기가 원래 사이즈였을 때도 똑같이 k-fold cross validation (k=5)를 진행해서 두 경우의 validation loss (k round의 평균 test loss)를 비교해보세요.\n",
        "\n",
        "###참고사항\n",
        "1.   위에 주어진 코드에서는 학습 방법에 초점이 맞춰져 test set이 따로 없었지만, 실제로 학습을 진행할때는 validation set이나 test set을 통해 성능을 확인하는 것이 필수적입니다!\n",
        "2.   전체 데이터를 k개의 부분집합으로 나누기 위해서 아래와 같은 sklearn 라이브러리의 KFold 클래스를 사용해도 되지만,\n",
        "```\n",
        "from sklearn.model_selection import KFold\n",
        "```\n",
        "연습을 위해서 Lab 1-1에서 배운 Slicing을 통해서 데이터셋을 k개로 나누고\n",
        "k round의 learning을 진행해주세요.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxJIfzq0Zwzi"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpkizQoxmE6V",
        "outputId": "e3aa03cf-7122-4ce2-e39c-4309de01d82e"
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENvL0yB0lEig"
      },
      "source": [
        "file = open('/content/drive/MyDrive/Colab Notebooks/data/regression_data.txt','r')  # open the file with read-only\n",
        "text = file.readlines()  # read all line texts\n",
        "file.close()  # close the file\n",
        "\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "# convert to float\n",
        "for idx,s in enumerate(text):\n",
        "    if idx%10==0:\n",
        "      data = s.split()\n",
        "      x_data.append(float(data[0]))\n",
        "      y_data.append(float(data[1]))    \n",
        "\n",
        "# convert to numpy-array\n",
        "x_data = np.asarray(x_data, dtype=np.float32)\n",
        "y_data = np.asarray(y_data, dtype=np.float32)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "vRQEu4LMlfYF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f0aa675d-26a5-4d5b-9730-d627a0555bf2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(x_data, y_data, 'ro') # plot data\n",
        "\n",
        "plt.xlabel('x-axis')  \n",
        "plt.ylabel('y-axis')\n",
        "plt.title('My data')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tkh5jXIlfYH"
      },
      "source": [
        "### Gradient Descent로 Linear Regression (K-fold cross validation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqJGz83SY3z1",
        "outputId": "074769a1-2df4-4245-bc53-4bcbd02fe3fb"
      },
      "source": [
        "# data shaping\n",
        "if len(x_data.shape)==1 and len(y_data.shape)==1:\n",
        "  x_data = np.expand_dims(x_data, axis=-1)\n",
        "  y_data = np.expand_dims(y_data, axis=-1)\n",
        "print(x_data.shape, y_data.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 1) (10, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klDE7E0Pnpho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c25fba99-3557-4601-b712-2875af437126"
      },
      "source": [
        "# Hyper-parameters\n",
        "input_size = 1\n",
        "output_size = 1\n",
        "num_epochs = 100\n",
        "learning_rate = 0.1\n",
        "K=5\n",
        "n_over_k = int(len(x_data)/K)\n",
        "\n",
        "val_losses = [] # array for saving validation losses\n",
        "\n",
        "for k in range(K):\n",
        "  # Linear regression model, y = Wx+b\n",
        "  model = nn.Linear(input_size, output_size) \n",
        "\n",
        "  # Loss and optimizer\n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "  \n",
        "  # training / validation partition of the k-th round\n",
        "  # np.delete 함수에 slice를 넘겨주면 해당 axis의 해당 인덱스들을 array에서 제거\n",
        "  x_train = np.delete(x_data, slice(k*n_over_k,(k+1)*n_over_k), axis=0) \n",
        "  y_train = np.delete(y_data, slice(k*n_over_k,(k+1)*n_over_k), axis=0)\n",
        "\n",
        "  # 앞 뒤로 slice 한것을 첫 번째 axis를 기준으로 붙여도 됨(concatenate 함수)\n",
        "  # x_train = np.concatenate((x_data[:k*n_over_k,:],x_data[(k+1)*n_over_k:,:]), axis=0) \n",
        "  # y_train = np.concatenate((y_data[:k*n_over_k,:],y_data[(k+1)*n_over_k:,:]), axis=0) \n",
        "\n",
        "  x_valid = x_data[k*n_over_k:(k+1)*n_over_k, :]\n",
        "  y_valid = y_data[k*n_over_k:(k+1)*n_over_k, :]\n",
        "\n",
        "  # Train the model\n",
        "  for epoch in range(num_epochs):\n",
        "      # Convert numpy arrays to torch tensors\n",
        "      inputs = torch.from_numpy(x_train)\n",
        "      targets = torch.from_numpy(y_train)\n",
        "\n",
        "      # Predict outputs with the linear model.\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets)\n",
        "      \n",
        "      # compute gradients and update\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      if (epoch+1) % 20 == 0:\n",
        "          print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "  # validation error\n",
        "  inputs = torch.from_numpy(x_valid)\n",
        "  targets = torch.from_numpy(y_valid)\n",
        "  outputs = model(inputs)\n",
        "  loss = criterion(outputs, targets)\n",
        "  print(k+1,\"-th round validation error: \",loss.item())\n",
        "  val_losses.append(loss.item())\n",
        "\n",
        "val_losses = np.asarray(val_losses)\n",
        "print(\"Final validation error: \", val_losses.mean())\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/100], Loss: 0.0129\n",
            "Epoch [40/100], Loss: 0.0009\n",
            "Epoch [60/100], Loss: 0.0003\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "1 -th round validation error:  8.737650932744145e-05\n",
            "Epoch [20/100], Loss: 0.0056\n",
            "Epoch [40/100], Loss: 0.0009\n",
            "Epoch [60/100], Loss: 0.0003\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "2 -th round validation error:  9.75363036559429e-06\n",
            "Epoch [20/100], Loss: 0.0015\n",
            "Epoch [40/100], Loss: 0.0003\n",
            "Epoch [60/100], Loss: 0.0002\n",
            "Epoch [80/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "3 -th round validation error:  0.00042456871597096324\n",
            "Epoch [20/100], Loss: 0.0371\n",
            "Epoch [40/100], Loss: 0.0039\n",
            "Epoch [60/100], Loss: 0.0005\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "4 -th round validation error:  0.0002807899727486074\n",
            "Epoch [20/100], Loss: 0.0021\n",
            "Epoch [40/100], Loss: 0.0004\n",
            "Epoch [60/100], Loss: 0.0002\n",
            "Epoch [80/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "5 -th round validation error:  0.0007952687446959317\n",
            "Final validation error:  0.0003195515146217076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40BbiXRSgnIa"
      },
      "source": [
        "file = open('/content/drive/MyDrive/Colab Notebooks/data/regression_data.txt','r')  # open the file with read-only\n",
        "text = file.readlines()  # read all line texts\n",
        "file.close()  # close the file\n",
        "\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "# convert to float\n",
        "for s in text:\n",
        "    data = s.split()\n",
        "    x_data.append(float(data[0]))\n",
        "    y_data.append(float(data[1]))    \n",
        "\n",
        "# convert to numpy-array\n",
        "x_data = np.asarray(x_data, dtype=np.float32)\n",
        "y_data = np.asarray(y_data, dtype=np.float32)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5v7svQTehMU",
        "outputId": "0468ad46-eda7-4ed3-f49b-297c0a42a9b2"
      },
      "source": [
        "# data shaping\n",
        "if len(x_data.shape)==1 and len(y_data.shape)==1:\n",
        "  x_data = np.expand_dims(x_data, axis=-1)\n",
        "  y_data = np.expand_dims(y_data, axis=-1)\n",
        "print(x_data.shape, y_data.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 1) (100, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NB4hIS2bekTp",
        "outputId": "9718fe8f-5b64-44c0-b3ea-da572faca793"
      },
      "source": [
        "# Hyper-parameters\n",
        "input_size = 1\n",
        "output_size = 1\n",
        "num_epochs = 100\n",
        "learning_rate = 0.1\n",
        "K=5\n",
        "n_over_k = int(len(x_data)/K)\n",
        "\n",
        "val_losses = [] # array for saving validation losses\n",
        "\n",
        "for k in range(K):\n",
        "  # Linear regression model, y = Wx+b\n",
        "  model = nn.Linear(input_size, output_size) \n",
        "\n",
        "  # Loss and optimizer\n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "  \n",
        "  # training / validation partition of the k-th round\n",
        "  # np.delete 함수에 slice를 넘겨주면 해당 axis의 해당 인덱스들을 array에서 제거\n",
        "  x_train = np.delete(x_data, slice(k*n_over_k,(k+1)*n_over_k), axis=0) \n",
        "  y_train = np.delete(y_data, slice(k*n_over_k,(k+1)*n_over_k), axis=0)\n",
        "\n",
        "  # 앞 뒤로 slice 한것을 첫 번째 axis를 기준으로 붙여도 됨(concatenate 함수)\n",
        "  # x_train = np.concatenate((x_data[:k*n_over_k,:],x_data[(k+1)*n_over_k:,:]), axis=0) \n",
        "  # y_train = np.concatenate((y_data[:k*n_over_k,:],y_data[(k+1)*n_over_k:,:]), axis=0) \n",
        "  \n",
        "  x_valid = x_data[k*n_over_k:(k+1)*n_over_k, :]\n",
        "  y_valid = y_data[k*n_over_k:(k+1)*n_over_k, :]\n",
        "\n",
        "  # Train the model\n",
        "  for epoch in range(num_epochs):\n",
        "      # Convert numpy arrays to torch tensors\n",
        "      inputs = torch.from_numpy(x_train)\n",
        "      targets = torch.from_numpy(y_train)\n",
        "\n",
        "      # Predict outputs with the linear model.\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets)\n",
        "      \n",
        "      # compute gradients and update\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      if (epoch+1) % 20 == 0:\n",
        "          print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "  # validation error\n",
        "  inputs = torch.from_numpy(x_valid)\n",
        "  targets = torch.from_numpy(y_valid)\n",
        "  outputs = model(inputs)\n",
        "  loss = criterion(outputs, targets)\n",
        "  print(k+1,\"-th round validation error: \",loss.item())\n",
        "  val_losses.append(loss.item())\n",
        "\n",
        "val_losses = np.asarray(val_losses)\n",
        "print(\"Final validation error: \", val_losses.mean())\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/100], Loss: 0.0148\n",
            "Epoch [40/100], Loss: 0.0018\n",
            "Epoch [60/100], Loss: 0.0003\n",
            "Epoch [80/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "1 -th round validation error:  0.00021114009723532945\n",
            "Epoch [20/100], Loss: 0.0268\n",
            "Epoch [40/100], Loss: 0.0030\n",
            "Epoch [60/100], Loss: 0.0004\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "2 -th round validation error:  4.0141978388419375e-05\n",
            "Epoch [20/100], Loss: 0.0249\n",
            "Epoch [40/100], Loss: 0.0022\n",
            "Epoch [60/100], Loss: 0.0003\n",
            "Epoch [80/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "3 -th round validation error:  0.00016621171380393207\n",
            "Epoch [20/100], Loss: 0.0023\n",
            "Epoch [40/100], Loss: 0.0004\n",
            "Epoch [60/100], Loss: 0.0002\n",
            "Epoch [80/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "4 -th round validation error:  6.833169027231634e-05\n",
            "Epoch [20/100], Loss: 0.0006\n",
            "Epoch [40/100], Loss: 0.0002\n",
            "Epoch [60/100], Loss: 0.0001\n",
            "Epoch [80/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "5 -th round validation error:  0.00013786149793304503\n",
            "Final validation error:  0.00012473739552660846\n"
          ]
        }
      ]
    }
  ]
}